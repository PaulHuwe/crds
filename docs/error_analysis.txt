These notes summarize progress and status for informal comparison tests between
CDBS and CRDS.   The bulk of the work here centers around explaining
discrepancies between CRDS and CDBS given dataset input parameters taken from
the HST catalog and best reference outputs taken from the HST catalog and CRDS.
My intent is that the following test cases comprise all known datasets for
modern HST instruments.

In the tables below,  there are two broad classes of errors:  "mismatch errors" 
where CDBS and CRDS produce different results based on the same inputs,  and 
"rmaps with overlap" which represent an unexpected issue with defining rmaps.
The bulk of my debug efforts have centered around mismatch error analysis.

Here is the coarse of error status reported in March:   

'''Mapping status 2012-03'''

|| '''task'''         || '''mismatch errors''' || '''rmaps with overlap''' || 
||    ACS mappings    ||       1909 / 59006    ||         5 / 18           ||
||    COS mappings    ||       0  / 8877       ||         1 / 16           ||
||    NICMOS mappings ||    2393 / 116078      ||         1 / 18           ||
||    STIS mappings   ||     0  / 144711       ||         2 / 31           ||
||    WFPC2 mappings  ||    9  / 186480        ||         1 / 11           ||
||    WFC3 mappings   ||     0  / 34616        ||         3 / 12           ||


Here is the current status of CRDS debugging as of now:

'''Mapping status 2012-06'''


|| '''task'''         || '''mismatch errors''' || '''rmaps with overlap''' || 
||    ACS mappings    ||     1277 / 41121      ||          6 / 19          ||
||    COS mappings    ||     1038  / 4752      ||          1 / 18          ||
||    NICMOS mappings ||     781 / 116078      ||          1 / 17          ||
||    STIS mappings   ||     0  / 146319       ||          2 / 31          ||
||    WFPC2 mappings  ||     11  / 186480      ||          1 / 10          ||
||    WFC3 mappings   ||     1243  / 37483     ||          3 / 11          ||


Further categorizing the mismatch errors:

ACS  mostly pfltfile, biasfile,  some darkfile 

COS  all 1038 new errors related to a CDBS change in the handling of xtractab.
     all the errors appear to be catalog related.
     
NICMOS notable progress using MAST data

WFC3 discovery of masked errors and completion of special case code

WFPC2  11 errors in 2 equivalence classes



Mismatch Errors
---------------

The consensus for addressing the errors known in March was that there might be
discrepancies between the CDBS answers reported in the catalog and the best
computable answers available to OPUS, OTFR, and ultimately MAST.  In other
words,  somewhat infrequently the catalog answers for CDBS are either wrong or
out of date.  To compensate,  an effort was made to download selected datasets
from MAST in an attempt to provide more accurate CDBS best references.  It has
been noted that the possibility for error here runs two ways:  first,  CRDS and 
CDBS might mismatch because the catalog is inaccurate.  second,  CRDS and CDBS
might *match* when they really do not,  because the catalog is inaccurate.   This
latter case is deemed less likely and ignored,   but we note the possibility as
one of the limitations of catalog based testing.

In order to minimize the amount of MAST data processing requested,  an attempt
was made to group errors into equivalence sets requiring only a single MAST
dataset to resolve.   Initially I grouped errors on the basis of "outputs",
i.e. a comparision tuple:  (cdbs(x),   crds(x)).   That grouping basis is only
heuristically correct.   The correct basis for equating datasets is by the input
parameters "x",  not the output of the functions.  Here note that the input
variable "x" is complex and corresponds to as many as 8 or 10 parameters which
differ by instrument and reference file type.

Further refining the grouping of errors by inputs,  it became evident that "x"
is actually different for all datasets because of the universal inclusion of
exposure start time. Reluctant to give up,   I imployed the original "same
outputs" heuristic and ignored time,  essentially keying the final equivalence
on (x, cdbs(x), crds(x)) where "x" does not include time.   While I believe this
is fairly good,  even this basis of equivalence is approximate...  because of
the exclusion of time I may be smearing too many similar x's together.

It should be pointed out that efforts at improved testing with MAST data did
result in the reduction of some error counts, most notably for NICMOS. Further,
adhoc examination of test cases resulted in the discovery of a few CRDS bugs.
Nevertheless, compared to initial expectations the hard numbers reveal a 
disappointing level of progress.


my recommendation will remain the same:  we
should abandon using MAST as a source of corrected CDBS best reference answers
and switch to more direct access to CDBS as it runs in OPUS.   Ultimately what
is needed is an accurate function, cdbs(x).

Factors Impeding Error Reduction
................................

1. One of the problems with the "honest" grouping of dataset inputs into 
better equivalence classes is that the granularity descreased:  there are more
and smaller equivalence sets,  which runs counter to the requirement
of minimizing MAST datasets downloaded.   The net result is that it is 
impractical to get improved answers for all of the equivalence classes:  there 
are now (probably) too many.   On the flip side, the testing is now more 
rigorous, and less likely to hide errors.  The real problem though is that we're 
effectively saying "it's too expensive to get exact answers for all these cases
from MAST".

2. Further problems emerged in getting desired MAST data.   It wasn't always
clear which 3 character data extension or class should be used to define best
references.   MAST was not able to supply every requested dataset.   Sometimes
single datasets turned out to be associations and exploded into several other
datasets,  doubling or tripling the expected size of the original request.

3.  The core CRDS code was not as far along in March as believed.   A
bug and unimplemented special case code hid errors for WFC3.

4.  CDBS is a moving target.   The errors now seen in COS are all related to a
single modest change to COS xtractab made in CDBS.   At present,  these new 
"errors" are being absorbed randomly without notice.   An improved mechanism 
for delivering key CDBS files and changfes to CRDS needs to be devised.

5. In general,  not getting exact answers from CDBS the first time around is
greatly complicating testing.   First,  getting "answers" becomes a two step
process.   Second,  many additional opportunities for error are introduced:
were the correct equivalence classes created?  were the equivalence classes
applied correctly?  did MAST deliver the files?


Overlaps
--------

In principle,  for JWST overlaps can be avoided by careful incremental
construction of rmaps,  preferrably with human oversight to assure simplicity and
minimality.  In practice,  for HST,  overlaps are a direct result of intentional
or unintentional special case handling of CDBS reference files as recorded in
the database.

I believe the majority/all of the HST ambiguities can be resolved by dynamically
merging all matching rules for a set of inputs... effectively emulating the
behavior of CDBS which also considers all of the same cases.  An alternative to
tolerating ambiguities is for human beings to examine the rmaps and weed out any
accidental or expendable complexity which is found in the CDBS database.  Unless
the database itself is "fixed" however, manually simplifying the rmaps becomes
an obstacle to automatically updating the rmaps from the database.

