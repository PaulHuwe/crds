These notes summarize progress and status for informal comparison
tests between CDBS and CRDS.  The work centers around explaining
discrepancies between CRDS and CDBS given dataset input parameters
taken from the HST catalog and best reference outputs taken from the
HST catalog and CRDS.  My intent is that the following test cases
comprise all known datasets for modern HST instruments.

In the tables below, the column "mismatch errors" shows errors
versus the total number of dataset comparisons.

Status reported in March:   

|| '''task'''         || '''mismatch errors''' || 
||    ACS mappings    ||       1909 / 59006    ||
||    COS mappings    ||       0  / 8877       ||
||    NICMOS mappings ||    2393 / 116078      ||
||    STIS mappings   ||     0  / 144711       ||
||    WFPC2 mappings  ||    9  / 186480        ||
||    WFC3 mappings   ||     0  / 34616        ||

Status now:

|| '''task'''         || '''mismatch errors''' ||
||    ACS mappings    ||     1277 / 41121      ||
||    COS mappings    ||     1038  / 4752      ||
||    NICMOS mappings ||     781 / 116078      ||
||    STIS mappings   ||     0  / 146319       ||
||    WFPC2 mappings  ||     11  / 186480      ||
||    WFC3 mappings   ||     1243  / 37483     ||


Further categorizing mismatch errors:
-------------------------------------

ACS    mostly pfltfile, biasfile,  some darkfile 

COS    all errors are related to a CDBS change in the handling of xtractab
	   the catalog is now out of date.
     
NICMOS notable progress using MAST data

WFC3   discovery of masked errors and missing special case code in CRDS

WFPC2  11 errors in 2 equivalence classes

In general:  reductions in overall dataset counts were due to discovery 
   			 and removal of duplicate datasets.

Both ACS and WFC3 have special case code involved.

Difficulty with using MAST
--------------------------

1.  Requirement to minimize MAST dataset downloads necessitates defining
    dataset equivalence sets where any dataset in an ES is suitable for
    representing the CDBS answers of all members of the set.   This was
    complicated,  initially done wrong, and ultimately performed on a 
	per-reference-type basis.   Even "done right" no two datasets are
	trivially equivalent because they have different exposure start
	times;   so my best effort at defining equivalence still has a 
	measure of doubt involved.

2.  Asking for datasets, there is ambiguity about which extension to ask
    for,  and ambiguity about where exactly the corrected CDBS answers
    are.   How does one get an updated dataset and be sure it is updated?

3.  Asking for MAST datasets,  they're not always delivered.   This
    may be a problem in my interpretation of the database and definition
    of "dataset".  Members of any associations are also included,  and 
    sometimes the reference files themselves are included;  both vastly
    expand the size of the original request without any confirmation.

4.  When datasets are successfully delivered, there's additional
    complexity correctly applying them as surrogates for others in the
    same equivalence set.

In summary,  working with the catalog and attempting to enhance it with MAST
datasets,  there are problems or complexity with:

- defining the unique datasets which should be tested/compared
  (duplicates and associations)

- defining equivalent dataset error classes (which erring datasets
  represent the same case?  did I define "equivalent" right?)

- obtaining large numbers of representative MAST datasets (It's hard
  to track all the failures.   Sometimes MAST is saying "no".)

- applying equivalent datasets as substitute answers (did I use the
  new MAST data when I should have?)

- knowing whether or not the correct data was requested from MAST
  (I asked MAST for *something*.  Does it actually have better answers?
  What types of files am I supposed to be asking for?  I want to ask
  for a "dataset" but I'm given a list of 5-6 obscure file types to
  choose from as well.)

- afterward, knowing whether an error case has a MAST correction
  there are now two kinds of answers,  corrected and uncorrected.  
  complicated.

- getting corrections for all small equivalence classes is a lot
  of files.   So far I haven't asked for them all.

- implementing equivalence classes required calling CRDS differently.

These are all sources of potential error and implementation work.
Consequently we have these questions:

1. Did CRDS give the wrong answer?
2. Did CDBS give the wrong answer?

but these extra questions:

3. Was it a valid dataset?
4. Did I accurately characterize equivalent datasets?
5. Did I ask for an improved MAST answer?
6. Did I get the answer I asked for?
7. Did I correctly apply the answer I got?

At intervals,  the answers to 3-7 are "no".

Although I was initially optimistic about the MAST approach, it has
gotten so complicated that I'll be less confident about the results
even if it eventually appears to work.  As a remedy, I think we need
access to the OPUS environment after all, the sooner, the better.

What we need in the simplest possible fashion is:

1. An accurate stream of dataset ids.  What *are* the datasets?  No
duplicates.  No associations.

2. For each dataset,  what are the best ref parameters?   Alternately,
give us the .fits file somewhere.

3. For each dataset, what are the CDBS best reference recommendations?

